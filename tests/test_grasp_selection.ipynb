{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "def reload_modules(*modules):\n",
    "    for module in modules:\n",
    "        if module.__name__ in sys.modules:\n",
    "            importlib.reload(module)\n",
    "        else:\n",
    "            print(f\"Module {module.__name__} is not currently imported.\")\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import ast\n",
    "import importlib\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "# reload_modules(pcd, utils, real, ur5, gripper, control_utils)\n",
    "from magpie_perception import pcd\n",
    "from magpie_perception import utils\n",
    "from magpie_perception.utils import label_wrist_image, find_object, label_wrist_image_rotation_axes, optimize_and_correct_frame, plot_frames_3d\n",
    "from magpie_control import realsense_wrapper as real\n",
    "from magpie_control import ur5\n",
    "from magpie_control import gripper\n",
    "from magpie_control import poses\n",
    "from magpie_control import utils as control_utils\n",
    "import spatialmath as sm\n",
    "reload_modules(pcd, utils, real, ur5, gripper, control_utils)\n",
    "from magpie_control.utils import transform_6d\n",
    "\n",
    "# rerun viz\n",
    "# import rerun as rr\n",
    "# from rerun_rlds_ur5.rlds import RLDSDataset, DeliGraspTrajectory, LiveRobotData\n",
    "# from rerun_rlds_ur5.rerun_loader_urdf import URDFLogger, get_urdf_paths, update_urdf\n",
    "# reload_modules(LiveRobotData, URDFLogger)\n",
    "# ur5_urdf, _, _= get_urdf_paths(\"ur5\")\n",
    "# rr.init(\"ScalingForce-visualized\", spawn=True)\n",
    "# urdf_logger = URDFLogger(ur5_urdf)\n",
    "# urdf_logger.log()\n",
    "# rr.log(\"annotation\", rr.TextDocument(\"annotation_1\",media_type=\"text/markdown\"))\n",
    "# viz_joints = False\n",
    "# rerun_viz = LiveRobotData(entity_to_transform=urdf_logger.entity_to_transform, visualize_joints=viz_joints)\n",
    "# rr.send_blueprint(rerun_viz.blueprint())\n",
    "rerun_viz = None\n",
    "# with help from: https://github.com/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb\n",
    "import os\n",
    "from google import genai\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import instructor\n",
    "from magpie_prompts.prompts import sf_force_thinker, sf_grasp_selection, sf_force_reflection, sf_position_thinker\n",
    "from magpie_prompts import conversation\n",
    "from magpie_prompts.conversation import openai_encode_image, build_messages, send_message\n",
    "reload_modules(sf_force_thinker, sf_grasp_selection, sf_position_thinker, conversation, sf_force_reflection)\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('CORRELL_API_KEY')\n",
    "GEMINI_MODEL_ID = \"gemini-2.0-flash\" # @param [\"gemini-1.5-flash-latest\",\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-pro-exp-03-25\"] {\"allow-input\":true}\n",
    "# OPENAI_MODEL_ID = \"gpt-4o-mini\"\n",
    "OPENAI_MODEL_ID = \"gpt-4.1-mini\"\n",
    "\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "force_thinker = sf_force_thinker.PromptForceThinker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = real.poll_devices()\n",
    "print(devices)\n",
    "wrist = real.RealSense(fps=15, w=640, h=480, device_name=\"D405\")\n",
    "wrist.initConnection(device_serial=devices['D405'])\n",
    "wkspc = real.RealSense(zMax=5, fps=6, w=640, h=480, device_name=\"D435\")\n",
    "wkspc.initConnection(device_serial=devices['D435'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"pick up the drill\"\n",
    "# user_query = \"push the bottle to the green duck\"\n",
    "# user_query = \"tip the bottle over onto its front side\"\n",
    "grasp_phrase = \"the drill handle\"\n",
    "\n",
    "pcd_wrist, rgbd_wrist = wrist.getPCD()\n",
    "pcd_wkspc, rgbd_wkspc = wkspc.getPCD()\n",
    "\n",
    "img_wrist = np.array(rgbd_wrist.color).copy()\n",
    "img_wkspc = np.array(rgbd_wkspc.color)\n",
    "img = img_wrist.copy()\n",
    "H0, W0 = img.shape[:2]\n",
    "img = Image.fromarray(img).copy()\n",
    "W1 = 800\n",
    "H1 = int(800 * H0 / W0)\n",
    "# img = Image.fromarray(img_wrist)\n",
    "\n",
    "img = img.resize((W1, H1), Image.Resampling.LANCZOS)\n",
    "# img_wkspc_resized = imutils.resize(img, width=1000)\n",
    "\n",
    "grasp_prompt = sf_grasp_selection.grasp_axis\n",
    "# grasp_prompt = sf_grasp_selection.antipodal_grasp\n",
    "image_response = gemini_client.models.generate_content(\n",
    "    model=GEMINI_MODEL_ID,\n",
    "    contents=[grasp_prompt, f\"task: {user_query} grasp phrase: {grasp_phrase}\", img],\n",
    ")\n",
    "grasp_selection = ast.literal_eval(image_response.text.split(\"\\n\")[1])\n",
    "points = grasp_selection['points']\n",
    "pt1, pt2 = points\n",
    "image_response.text\n",
    "\n",
    "def plot_norm_points_on_image(img, points, label=\"grasp\", color='red'):\n",
    "    plt.imshow(img)\n",
    "    for pt in points:\n",
    "        y_norm, x_norm = pt\n",
    "        y_px = (y_norm / 1000) * H1\n",
    "        x_px = (x_norm / 1000) * W1\n",
    "        scale_x = W0 / W1\n",
    "        scale_y = H0 / H1\n",
    "        x_orig = x_px * scale_x\n",
    "        y_orig = y_px * scale_y\n",
    "        plt.scatter(x_orig, y_orig, c=color, s=100, marker='o', edgecolors='black', linewidths=1.5)\n",
    "        # plt.text(x + 5, y - 5, label, color=color, fontsize=12, weight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_norm_points_on_image(img_wrist, points, label=\"grasp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_wrist, rgbd_wrist = wrist.getPCD()\n",
    "pcd_wkspc, rgbd_wkspc = wkspc.getPCD()\n",
    "\n",
    "img_wrist = np.array(rgbd_wrist.color).copy()\n",
    "img_wkspc = np.array(rgbd_wkspc.color)\n",
    "H0, W0 = img_wkspc.shape[:2]\n",
    "img = Image.fromarray(img_wkspc).copy()\n",
    "W1 = 800\n",
    "H1 = int(800 * H0 / W0)\n",
    "# img = Image.fromarray(img_wrist)\n",
    "\n",
    "img = img.resize((W1, H1), Image.Resampling.LANCZOS)\n",
    "img_wkspc_resized = imutils.resize(img_wkspc, width=1000)\n",
    "\n",
    "\n",
    "grasp_prompt = sf_grasp_selection.grasp_prompt\n",
    "image_response = gemini_client.models.generate_content(\n",
    "    model=GEMINI_MODEL_ID,\n",
    "    contents=[grasp_prompt, f\"task: {user_query} grasp phrase: {grasp_phrase}\", img],\n",
    ")\n",
    "grasp_selection = ast.literal_eval(image_response.text.split(\"\\n\")[1])\n",
    "point = grasp_selection['point']\n",
    "y_norm, x_norm = point\n",
    "\n",
    "# Step 1: [0, 1000] → resized pixel coordinates\n",
    "y_px = (y_norm / 1000) * H1\n",
    "x_px = (x_norm / 1000) * W1\n",
    "# Step 2: resized pixels → original pixels\n",
    "scale_x = W0 / W1\n",
    "scale_y = H0 / H1\n",
    "x_orig = x_px * scale_x\n",
    "y_orig = y_px * scale_y\n",
    "def plot_point_on_image(img, x, y, label=\"grasp\", color='red'):\n",
    "    plt.imshow(img)\n",
    "    plt.scatter([x], [y], c=color, s=100, marker='o', edgecolors='black', linewidths=1.5)\n",
    "    plt.text(x + 5, y - 5, label, color=color, fontsize=12, weight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# origin = (x_orig, y_orig)\n",
    "plot_point_on_image(img_wkspc, x_orig, y_orig, label=\"grasp\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
