{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'magpie_perception'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mo3d\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmagpie_perception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pcd\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmagpie_control\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m realsense_wrapper \u001b[38;5;28;01mas\u001b[39;00m real\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'magpie_perception'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def reload_modules(*modules):\n",
    "    for module in modules:\n",
    "        if module.__name__ in sys.modules:\n",
    "            importlib.reload(module)\n",
    "        else:\n",
    "            print(f\"Module {module.__name__} is not currently imported.\")\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "from magpie_perception import pcd\n",
    "from magpie_perception import utils\n",
    "from magpie_perception import utils\n",
    "from magpie_perception.utils import label_wrist_image, find_object\n",
    "from magpie_control import realsense_wrapper as real\n",
    "importlib.reload(pcd, utils, real, ur5)\n",
    "\n",
    "# from open3d.web_visualizer import draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D435': '832412070344', 'D405': '126122270157'}\n",
      "There are 2 available devices!\n",
      "There are 2 available devices!\n"
     ]
    }
   ],
   "source": [
    "devices = real.poll_devices()\n",
    "print(devices)\n",
    "wrist = real.RealSense(fps=15, w=640, h=480, device_name=\"D405\")\n",
    "wrist.initConnection(device_serial=devices['D405'])\n",
    "wkspc = real.RealSense(zMax=5, fps=6, w=640, h=480, device_name=\"D435\")\n",
    "wkspc.initConnection(device_serial=devices['D435'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_wrist, rgbd_wrist = wrist.getPCD()\n",
    "pcd_wkspc, rgbd_wkspc = wkspc.getPCD()\n",
    "\n",
    "img_wrist = np.array(rgbd_wrist.color)\n",
    "img_wkspc = np.array(rgbd_wkspc.color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(wrist_img, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+Z\u001b[39m\u001b[38;5;124m\"\u001b[39m, (center_x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m45\u001b[39m, center_y \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m60\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m2\u001b[39m, z_color, \u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrist_img\n\u001b[1;32m---> 31\u001b[0m img_wrist_rotated \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mrotate(img_wrist, cv2\u001b[38;5;241m.\u001b[39mROTATE_90_COUNTERCLOCKWISE)\n\u001b[0;32m     32\u001b[0m wrist_img_labeled \u001b[38;5;241m=\u001b[39m label_wrist_image(img_wrist_rotated\u001b[38;5;241m.\u001b[39mcopy()) \u001b[38;5;66;03m# Use .copy() to avoid modifying the original\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# img_wkspc_resized = cv2.resize(img_wkspc, (img_wrist_rotated.shape[1], img_wrist_rotated.shape[0]))\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "def label_wrist_image(wrist_img):\n",
    "    '''\n",
    "    labels wrist image with overlaid x,y,z axes in line with F/T sensor\n",
    "    '''\n",
    "    h, w, c = wrist_img.shape\n",
    "\n",
    "    x_color = (0, 255, 0) # green\n",
    "    y_color = (0, 0, 255) # blue\n",
    "    z_color = (255, 0, 0) # red\n",
    "\n",
    "    # Define axis line lengths (adjust as needed)\n",
    "    axis_length = min(w, h) // 3\n",
    "\n",
    "    # Calculate starting point for axes (center of the image)\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "\n",
    "    # Draw X-axis\n",
    "    cv2.arrowedLine(wrist_img, (center_x, center_y), (center_x + axis_length, center_y), x_color, 6)\n",
    "    cv2.putText(wrist_img, \"+X\", (center_x + axis_length - 15, center_y -20), cv2.FONT_HERSHEY_SIMPLEX, 2, x_color, 6)\n",
    "\n",
    "    # Draw Y-axis\n",
    "    cv2.arrowedLine(wrist_img, (center_x, center_y), (center_x, center_y - axis_length), y_color, 6)\n",
    "    cv2.putText(wrist_img, \"-Y\", (center_x - 15, center_y - axis_length - 5), cv2.FONT_HERSHEY_SIMPLEX, 2, y_color, 6)\n",
    "\n",
    "    # Draw Z-axis (pointing towards the viewer, represented as a circle)\n",
    "    cv2.circle(wrist_img, (center_x, center_y), 9, z_color, -1)  # Filled circle\n",
    "    cv2.putText(wrist_img, \"+Z\", (center_x - 45, center_y + 60), cv2.FONT_HERSHEY_SIMPLEX, 2, z_color, 6)\n",
    "\n",
    "    return wrist_img\n",
    "\n",
    "img_wrist_rotated = cv2.rotate(img_wrist, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "wrist_img_labeled = label_wrist_image(img_wrist_rotated.copy()) # Use .copy() to avoid modifying the original\n",
    "# img_wkspc_resized = cv2.resize(img_wkspc, (img_wrist_rotated.shape[1], img_wrist_rotated.shape[0]))\n",
    "import imutils\n",
    "img_wkspc_resized = imutils.resize(img_wkspc, height=img_wrist_rotated.shape[0])\n",
    "final_img = np.hstack((img_wkspc_resized, wrist_img_labeled))\n",
    "\n",
    "# save img\n",
    "fn = \"axes6.png\"\n",
    "cv2.imwrite(fn, cv2.cvtColor(final_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.axis('off')\n",
    "plt.imshow(final_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with help from: https://github.com/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb\n",
    "import os\n",
    "from PIL import Image\n",
    "from google import genai\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import instructor\n",
    "from force_prediction_prompts import initial_prompt, feedback_prompt\n",
    "from magpie_prompts.prompts import sf_force_thinker\n",
    "from magpie_prompts import conversation\n",
    "from magpie_prompts.conversation import openai_encode_image, build_messages, send_message\n",
    "from magpie_prompts.prompts import sf_grasp_selection\n",
    "import importlib\n",
    "importlib.reload(sf_force_thinker)\n",
    "importlib.reload(conversation)\n",
    "importlib.reload(sf_grasp_selection)\n",
    "\n",
    "user_query = \"grab the scissors\"\n",
    "user_query = \"grab the screwdriver\"\n",
    "user_query = \"open the box\"\n",
    "user_query = \"close the box\"\n",
    "\n",
    "grasp_prompt = sf_grasp_selection.grasp_prompt.format(**{\"user_query\": user_query})\n",
    "img = Image.fromarray(np.array(rgbd_wrist.color))\n",
    "image_response = gemini_client.models.generate_content(\n",
    "    model=GEMINI_MODEL_ID,\n",
    "    contents=[grasp_prompt, img],\n",
    ")\n",
    "\n",
    "image_response.text.split(\"\\n\")[-1]\n",
    "image_response.text.split(\"\\n\")[-1].split(\": \")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('CORRELL_API_KEY')\n",
    "GEMINI_MODEL_ID = \"gemini-2.0-flash\" # @param [\"gemini-1.5-flash-latest\",\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-pro-exp-03-25\"] {\"allow-input\":true}\n",
    "# OPENAI_MODEL_ID = \"gpt-4o-mini\"\n",
    "OPENAI_MODEL_ID = \"gpt-4o\"\n",
    "\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "force_thinker = sf_force_thinker.PromptForceThinker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"gemini\"\n",
    "# model_type = \"openai\"\n",
    "client, model = None, None\n",
    "if model_type == \"openai\":\n",
    "    client = openai_client\n",
    "    model = OPENAI_MODEL_ID\n",
    "elif model_type == \"gemini\":\n",
    "    client = gemini_client\n",
    "    model = GEMINI_MODEL_ID\n",
    "# img = Image.open('axes.png').convert(\"RGB\")\n",
    "# img = Image.open('axes4_obj-box-lid_motion-close.png').convert(\"RGB\")\n",
    "img = Image.open('axes3_obj-box-lid_motion-open.png').convert(\"RGB\")\n",
    "\n",
    "# obj = \"small servo motor\"\n",
    "# prompt_config = {\n",
    "# \"obj\": \"small servo motor\",\n",
    "# \"task\": f\"slide the {obj} rapidly up the table for 10 cm\"\n",
    "# }\n",
    "obj = \"case lid handle\"\n",
    "prompt_config = {\n",
    "\"obj\": obj,\n",
    "\"task\": f\"open the case by the {obj}\"\n",
    "}\n",
    "\n",
    "# prompt = initial_prompt.format(**prompt_config)\n",
    "messages = []\n",
    "prompt = sf_force_thinker.prompt_motion_thinker.format(**prompt_config)\n",
    "messages = build_messages(prompt, img, messages=messages, model_type=model_type)\n",
    "response = send_message(client, model, messages, model_type=model_type)\n",
    "motion_plan = force_thinker.process_thinker_response(response)\n",
    "\n",
    "\n",
    "# print(response)\n",
    "# print(response.split(\"```\"))\n",
    "# print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'motion_direction': [1, -1, 0], 'motion_goal': [0.05, 0.05, 0.0], 'force': [1.0, 0.75, 0.25], 'grasp_force': 2.5, 'duration': 2.0}\n"
     ]
    }
   ],
   "source": [
    "print(motion_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start of motion plan]\n",
      "The right image is labeled with the axes of motion, which do not necessarily correspond with the world axes.\n",
      "The task is to open the case by the case lid handle while grasping the case lid handle.\n",
      "The provided image confirms {DESCRIPTION: a blue plastic case sits on a grid surface. The case has a handle on the lid. A robot end-effector is positioned above the handle ready to grasp it. The task is to open the case by lifting the lid using the handle.}.\n",
      "The green axis pointing to the right of the image is the positive X-axis, corresponding {DESCRIPTION: to the primary direction of the lid opening. The lid will need to move in the positive X direction to open the case.}.\n",
      "The blue axis pointing up the image is the negative Y-axis, corresponding {DESCRIPTION: to a potential force component needed to maintain the grip on the handle and overcome any vertical friction or locking mechanisms. Minimal motion along Y is expected unless there is a latch.}.\n",
      "The red dot pointing into the image is the positive Z-axis, corresponding {DESCRIPTION: to no significant motion. However, the handle may require a slight force along the Z-axis to pull the handle towards the robot for a secure grip, or resist any backwards momentum}.\n",
      "\n",
      "Motion Required:\n",
      "X-axis: {DESCRIPTION: Movement is required in the positive X direction to open the case. The magnitude will depend on the desired opening angle.}\n",
      "\n",
      "Y-axis: {DESCRIPTION: Minimal movement along the negative Y axis is expected, potentially to overcome friction or latch mechanisms. It is mainly to keep the grip.}\n",
      "\n",
      "Z-axis: {DESCRIPTION: No significant movement is expected along the Z-axis. Some force along the negative Z axis may occur due to pulling to maintain contact}.\n",
      "\n",
      "To estimate the forces required to accomplish open the case by the case lid handle while grasping the case lid handle, we must consider the following:\n",
      "- {DESCRIPTION: The force required to overcome the static friction or latching mechanism that keeps the case closed. This will primarily act in the negative X direction.}\n",
      "- {DESCRIPTION: The force required to overcome the rotational inertia of the case lid around the hinge. This force can be estimated by knowing the mass, and size of the lid.}\n",
      "- {DESCRIPTION: The force required to maintain the grip on the handle while opening the case. This grip force will act along the Y and Z axes}.\n",
      "- {DESCRIPTION: Any resistance at the hinge, such as friction or damping, will require an additional force component to maintain rotational acceleration}.\n",
      "\n",
      "Qualitative Force Estimation:\n",
      "X-axis: {DESCRIPTION: 1-5 N. This force range is based on the assumption of a light plastic case and a simple friction-based latch or no latch at all. It's the force required to initiate opening against friction or latching.}.\n",
      "\n",
      "Y-axis: {DESCRIPTION: 0.5-2 N. This force is primarily to maintain contact with the handle against gravity and any vertical friction.}.\n",
      "\n",
      "Z-axis: {DESCRIPTION: 0.2-1 N. This force is primarily to pull the handle inwards towards the robot end effector to maintain a secure grip.}.\n",
      "\n",
      "Grasping force: {DESCRIPTION: 2-5 N. This force is to ensure a secure grip on the handle without slippage during the opening motion, depending on the surface friction between the gripper and handle.}.\n",
      "\n",
      "Detailed Force Estimation Reasoning:\n",
      "- Object Properties: {DESCRIPTION: Estimated mass of the lid: 0.1 kg. Material: Plastic. Assumed to be a light object with minimal stiffness. Friction coefficient between the case and lid latch (if any): 0.2-0.5. Assuming a simple hinge joint with minimal resistance. The lid is a simple rectangular shape.}.\n",
      "- Environmental Factors: {DESCRIPTION: Gravity is a factor, but the handle is assumed to be oriented such that gravity primarily acts along the Y-axis, requiring minimal Y-axis force to counteract gravity and a small additional contact force if the handle angle causes the lid to try to open or close itself due to gravity. Assume minimal surface friction on the table.}.\n",
      "- Contact Types: {DESCRIPTION: Pinch grasp on the handle. Maintaining surface contact between the gripper and handle. Edge contact between the case and the lid when initially opening.}.\n",
      "- Motion Type: {DESCRIPTION: Primarily rotational motion around the hinge. The handle will move in an arc. Initial application of force in X and Y to overcome latching and friction.}.\n",
      "- Contact Considerations: {DESCRIPTION: The robot must exert a force in the negative Y axis to maintain contact with the handle and potentially overcome vertical friction. A force may be required along the negative Z-axis to maintain inward contact on the handle to prevent slippage.}.\n",
      "- Motion along axes: {DESCRIPTION: The robot exerts motion in a “linear” fashion along the X axis and a “rotational” fashion along the [y, z] axes.}.\n",
      "\n",
      "Physical Model (if applicable):\n",
      "- Let's approximate the initial force needed to overcome static friction at the latch (if present).\n",
      "- F_friction = μ * N, where μ is the friction coefficient and N is the normal force (related to the weight of the lid acting on the latch).\n",
      "- Assuming N is approximately 0.5 N (lid weight component), and μ is 0.5, F_friction = 0.25 N.\n",
      "- To open the lid, the robot also needs to apply torque τ around the hinge. τ = I * α.\n",
      "- Assuming moment of inertia I is small (0.001 kg*m^2) and angular acceleration α is low (1 rad/s^2), τ = 0.001 N*m.\n",
      "- Force required at the handle F = τ / r, where r is the distance from the hinge to the handle (e.g., 0.1 m), F = 0.01 N. This is very small, meaning we are primarily overcoming friction or a latch, and inertia is negligible at low speeds.\n",
      "\n",
      "Final Computed Estimated Forces:\n",
      "X-axis: [1.0–5.0] N  \n",
      "Y-axis: [0.5–2.0] N  \n",
      "Z-axis: [0.2–1.0] N  \n",
      "Grasp: [2.0–5.0] N  \n",
      "\n",
      "Python Code:\n",
      "```python\n",
      "# describe the motion along the [x, y, z] axes as either positive, negative, or no motion\n",
      "motion_direction = [1, -1, 0]\n",
      "# resolve the magnitude of motion across the motion direction axes [x, y ,z]\n",
      "motion_goal = [0.15, 0.0, 0.0]\n",
      "# explicitly state the forces along the [x, y, z] axes\n",
      "force = [3.0, 1.0, 0.5]\n",
      "# explicitly state the grasping force, which must be positive\n",
      "grasp_force = 3.5\n",
      "# explicitly state the task duration, which must be positive\n",
      "duration = 2.0\n",
      "```\n",
      "\n",
      "[end of motion plan]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'motion_direction': [1, -1, 0],\n",
       " 'motion_goal': [0.15, 0.0, 0.0],\n",
       " 'force': [3.0, 1.0, 0.5],\n",
       " 'grasp_force': 3.5,\n",
       " 'duration': 2.0}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "# remove strings from list that begin with # or are empty\n",
    "motion_plan_string = str(response).split(\"python\")[-1].split(\"```\")[0].split('\\n')\n",
    "motion_plan_string = [x for x in motion_plan_string if x and not x.startswith(\"#\")]\n",
    "motion_plan_string = [x.strip() for x in motion_plan_string]\n",
    "# separate variable names and values from strings, make motion_plan dict\n",
    "motion_plan = {x.split(\"=\")[0].strip(): ast.literal_eval(x.split(\"=\")[1]) for x in motion_plan_string}\n",
    "motion_plan\n",
    "'''\n",
    "['motion_direction = [1, -1, 0]',\n",
    " 'motion_goal = [3.0, 6.0, 0.0]',\n",
    " 'force = [3.5, 7.5, 0.0]',\n",
    " 'grasp_force = 4.5',\n",
    " 'duration = 5.0']\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magpie_prompts.schemas import force_prediction_schema as fps\n",
    "import importlib\n",
    "importlib.reload(fps)\n",
    "\n",
    "# input=[{\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": [\n",
    "#         {\"type\": \"text\", \"text\": f\"{prompt}\"},\n",
    "#         {\n",
    "#             \"type\": \"image_url\",\n",
    "#             \"image_url\": f\"data:image/jpeg;base64,{openai_encode_image(img)}\",\n",
    "#         },\n",
    "#     ],\n",
    "# }]\n",
    "input=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        prompt,\n",
    "        instructor.Image.from_raw_base64(openai_encode_image(img)),\n",
    "    ],\n",
    "}]\n",
    "\n",
    "openai_client_structured = instructor.from_openai(client=openai_client)\n",
    "response = openai_client_structured.chat.completions.create(\n",
    "    messages=input,\n",
    "    response_model=force_prediction_schema.MotionForcePlan,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MotionForcePlan' object has no attribute 'goal_force'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoal_force\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pydantic\\main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MotionForcePlan' object has no attribute 'goal_force'"
     ]
    }
   ],
   "source": [
    "response.goal_force\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To complete the task of sliding the small servo motor rapidly up the table for 10 cm, we'll estimate the motion direction, displacement, required forces, time, and grasping force.\n",
      "\n",
      "### **Step 1: Initial Motion and Force Plan**\n",
      "\n",
      "1. **Direction of Motion:**  \n",
      "   Since we want to slide the servo motor up the table, this corresponds to moving in the +X direction based on the labeled axes.\n",
      "   ```python\n",
      "   direction = [1, 0, 0]  # +X direction\n",
      "   ```\n",
      "\n",
      "2. **Goal Position:**  \n",
      "   The target displacement is 10 cm in the +X direction.\n",
      "   ```python\n",
      "   goal_position = [0.1, 0, 0]  # 0.1 meters in +X\n",
      "   ```\n",
      "\n",
      "3. **Goal Force:**  \n",
      "   - **Mass Estimation:** Assume the small servo motor has a mass of approximately 0.05 kg.  \n",
      "   - **Friction:** Coefficient of friction is assumed to be around 0.3 for a typical surface.  \n",
      "   - **Force Calculation:**\n",
      "     - Normal Force, \\( F_N = m \\cdot g = 0.05 \\cdot 9.81 = 0.4905 \\) N  \n",
      "     - Frictional Force, \\( F_f = \\mu \\cdot F_N = 0.3 \\cdot 0.4905 \\approx 0.1472 \\) N  \n",
      "     - **Total Force Required:** Need to exceed \\( F_f \\) for motion, ensuring slightly more force for rapid motion:\n",
      "       \\[\n",
      "       F_x = 0.2 \\, \\text{N}  \\text{ (considering rapid motion)}\n",
      "       \\]\n",
      "     - No vertical or Z-axis forces except normal force balancing gravity.  \n",
      "   ```python\n",
      "   goal_force = [0.2, 0, 0]  # Only +X force, 0.2 N\n",
      "   ```\n",
      "\n",
      "4. **Estimated Task Duration:**  \n",
      "   - Assume rapid motion takes approximately 1 second for this displacement.  \n",
      "   ```python\n",
      "   task_duration = 1  # 1 second\n",
      "   ```\n",
      "\n",
      "5. **Estimated Grasping Force:**  \n",
      "   - **Grasp Strategy:** Firm enough to hold without damaging the object.\n",
      "   - **Force Calculation:** Using typical robotic gripping:\n",
      "     \\[\n",
      "     f_{\\text{grasp}} = 2 \\times F_f = 2 \\times 0.1472 \\approx 0.3 \\, \\text{N}\n",
      "     \\]\n",
      "   ```python\n",
      "   grasp_force = 0.3  # 0.3 N for secure grip\n",
      "   ```\n",
      "\n",
      "### **Final Output Format**\n",
      "\n",
      "```python\n",
      "direction = [1, 0, 0]  # -1, 1, or 0  \n",
      "goal_position = [0.1, 0, 0]  # in meters  \n",
      "goal_force = [0.2, 0, 0]  # in Newtons  \n",
      "task_duration = 1  # in seconds  \n",
      "grasp_force = 0.3  # in Newtons  \n",
      "```\n",
      "\n",
      "### **Rules for Generating Motion and Forces**\n",
      "\n",
      "- **Axes References**: Always align with the labeled wrist axes: +X is rightward, +Y is upward, +Z is forward.\n",
      "- **Physical Properties**: Consider friction, weight, and required forces for rapid yet controlled movement. Adjust for additional forces for stability and secure handling.\n"
     ]
    }
   ],
   "source": [
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I understand the setup and the requested format. Here's the analysis for the desired motion:\n",
      "\n",
      "**Desired motion description:** Grasp the {obj}\n",
      "\n",
      "**Analysis:**\n",
      "\n",
      "1.  **Coarse Motion:**\n",
      "    *   X: 0 (No significant motion along the X-axis is required for grasping. The gripper is aligned on that axis)\n",
      "    *   Y: -1 (The gripper needs to move down towards the object.)\n",
      "    *   Z: 0 (No significant motion along the Z-axis is required for grasping. The gripper is aligned on that axis)\n",
      "\n",
      "2.  **Fine Motion:**\n",
      "    *   X: 0.00 (No significant motion)\n",
      "    *   Y: -0.05 (Estimate the distance to move down based on the visible gap between the gripper and the object. An estimated 5cm should be enough)\n",
      "    *   Z: 0.00 (No significant motion)\n",
      "\n",
      "3.  **Coarse Forces:**\n",
      "    *   X: 0 (No significant forces along the X-axis are expected initially)\n",
      "    *   Y: 1 (A positive force (upwards) will be necessary to counteract gravity once the object is grasped)\n",
      "    *   Z: 0 (No significant forces along the Z-axis are expected initially)\n",
      "\n",
      "4.  **Fine Forces:**\n",
      "    *   X: 0.0 (Assume minimal friction on the X-axis)\n",
      "    *   Y: 0.5 (Estimate the force required to counteract gravity and hold the object. Based on the object's small size, 0.5 N is an educated guess)\n",
      "    *   Z: 0.0 (Assume minimal friction on the Z-axis)\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "1.  **Coarse Motion:** `[0, -1, 0]`\n",
      "2.  **Fine Motion:** `[0.00, -0.05, 0.00]`\n",
      "3.  **Coarse Forces:** `[0, 1, 0]`\n",
      "4.  **Fine Forces:** `[0.0, 0.5, 0.0]`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(image_response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
